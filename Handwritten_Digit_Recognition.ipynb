{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfAQrhmCBIs670b1ZnA1MY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meena-2826/Handwritten-Digit-Recognition/blob/main/Handwritten_Digit_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P05F5vZxAO8u",
        "outputId": "8d3a1559-882c-40b3-e633-3aee7b7ebd35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50, Loss: 0.2875\n",
            "Epoch 10/50, Loss: 0.2264\n",
            "Epoch 15/50, Loss: 0.1885\n",
            "Epoch 20/50, Loss: 0.1612\n",
            "Epoch 25/50, Loss: 0.1407\n",
            "Epoch 30/50, Loss: 0.1248\n",
            "Epoch 35/50, Loss: 0.1120\n",
            "Epoch 40/50, Loss: 0.1015\n",
            "Epoch 45/50, Loss: 0.0928\n",
            "Epoch 50/50, Loss: 0.0855\n",
            "Test Accuracy: 96.53%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist.data, mnist.target.astype(int)\n",
        "\n",
        "# Normalize the input data\n",
        "X = X / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)  # Updated for newer versions\n",
        "y_onehot = encoder.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize network parameters\n",
        "input_size = 784  # 28x28 pixels\n",
        "hidden_size = 128  # Increased hidden layer size\n",
        "output_size = 10  # Digits 0-9\n",
        "\n",
        "# Xavier Initialization\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Activation function: ReLU\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Activation function: Softmax\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X):\n",
        "    global Z1, A1, Z2, A2\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return A2\n",
        "\n",
        "# Loss function: Cross-entropy\n",
        "def compute_loss(y_true, y_pred):\n",
        "    n_samples = y_true.shape[0]\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / n_samples\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(X, y_true, y_pred):\n",
        "    global W1, b1, W2, b2\n",
        "    n_samples = X.shape[0]\n",
        "\n",
        "    # Gradients for output layer\n",
        "    dZ2 = y_pred - y_true\n",
        "    dW2 = np.dot(A1.T, dZ2) / n_samples\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / n_samples\n",
        "\n",
        "    # Gradients for hidden layer\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / n_samples\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / n_samples\n",
        "\n",
        "    # Update weights\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "# Training the model with mini-batch gradient descent\n",
        "epochs = 50  # Increased epochs\n",
        "batch_size = 64\n",
        "learning_rate = 0.01  # Reduced learning rate for smoother convergence\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        # Get mini-batch\n",
        "        X_batch = X_train[i:i + batch_size]\n",
        "        y_batch = y_train[i:i + batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = forward_pass(X_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        backward_pass(X_batch, y_batch, y_pred)\n",
        "\n",
        "    # Compute loss after each epoch\n",
        "    y_pred_train = forward_pass(X_train)\n",
        "    loss = compute_loss(y_train, y_pred_train)\n",
        "\n",
        "    # Print loss\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_test_pred = forward_pass(X_test)\n",
        "y_test_labels = np.argmax(y_test_pred, axis=1)\n",
        "y_true_labels = np.argmax(y_test, axis=1)\n",
        "accuracy = accuracy_score(y_true_labels, y_test_labels)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ]
}